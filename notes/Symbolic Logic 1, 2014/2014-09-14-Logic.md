---
course_title: Symbolic Logic 1
note_title: Week 3
has_toc: Yes
css: logic.css
---

#Derivations

Suppose we wish to know whether
the argument 

$P\rightarrow Q~.~Q\rightarrow R~.~R\rightarrow S~.~ S\rightarrow T\therefore P\rightarrow T$ 

is valid. We might reason like this. We do know that the simpler argument

$P\rightarrow Q~.~Q\rightarrow R\therefore P\rightarrow R$

is formally valid. The formal validity of this argument means that we know that when $P\rightarrow
Q$ and $Q\rightarrow R$ are true, then $P\rightarrow R$ must be true as well
(call this "Fact 1"). And because the argument is *formally* valid, we
know that other arguments of the same form are also deductively valid. For
example, $R\rightarrow S~.~S\rightarrow T\therefore R\rightarrow T$ is
deductively valid (call this "Fact 2"). So, we know that when $R\rightarrow S$
and $S\rightarrow T$ are true, $R\rightarrow T$ is true. We also know, on the
same basis, that when $P\rightarrow R$ and $R\rightarrow T$ are true, then
$P\rightarrow T$ is true (call this "Fact 3").

Why the blizzard of letters? What is the value of all this? Here is how this
helps us. In a situation where the premises $P\rightarrow Q, Q\rightarrow R,
R\rightarrow S, S\rightarrow T$ are all true, we know that $P\rightarrow R$ must
be true (by Fact 1). And, we also know that $R\rightarrow T$ must be true (by
Fact 2). So, we know that $P\rightarrow T$ must be true (by Fact 3).
Graphically, you can think of what's happening in this way:

<div style="display: table; margin: 0 auto">

```{#img2 .latex .tikz}
\begin{prooftree}
\AxiomC{$P\rightarrow Q$}
\AxiomC{$Q\rightarrow R$}\LeftLabel{Fact 1}
\BinaryInfC{$P\rightarrow R$}
\AxiomC{$R\rightarrow S$}
\AxiomC{$S\rightarrow T$}\RightLabel{Fact 2}
\BinaryInfC{$R\rightarrow T$}\LeftLabel{Fact 3}
\BinaryInfC{$P\rightarrow T$}
\end{prooftree}
```

</div>

When the premises on the top are all true, our knowledge of valid arguments
tells us that the premises on the second row must be true. But our knowledge of
valid arguments also tells us that when the premises on the second row are all
true, the conclusion at the bottom must be true. 

That means that when the premises at the top are all true, the conclusion at
the bottom must be true. And that is just to say that the argument

$P\rightarrow Q~.~Q\rightarrow R~.~R\rightarrow S~.~S\rightarrow T\therefore P\rightarrow T$

(the one we were wondering about) is valid. Truth flows through
the argument like water flowing through pipes, or electricity through wires.

This type of object---a series of arguments linked together---is called
a **direct derivation**. We'll introduce some special sorts of direct
derivations later on, so for now, let's just talk about the "simple" direct
derivations.

Direct Derivation
:   A (simple) *Direct Derivation* is a sequence of assertions, each of which is
    justified, either because it is a premise or because it is the conclusion of
    a valid and recognized argument based on previous assertions. 

In this section, we will learn a little bit about how to make use of direct
derivations. We will need to do two things. First, we need to decide which
valid arguments we will "recognize" in direct derivations. We need to decide
on some small set so that other people, who might not know the same arguments
as us, will be able to see the correctness of our derivations. Second, we need
to decide how to organize our derivations so that they will be readable and
easy to check for correctness.
 
Let's begin by deciding on which arguments we will "recognize" in direct
derivations.
 
##Rules of Direct Inference
 
Let's call the forms of argument that we will recognize in direct derivations
**rules of direct inference**. 

Rules of Direct Inference

:   A *rule of direct inference* is an argument form that we recognize as valid
    for purposes of constructing derivations

We do not yet know of any rules of indirect inference, but we will encounter
some eventually, which is why we have the qualification "direct" in "rules of
direct inference".
 
Two good argument forms to start with are Modus Ponens, and Modus Tollens. They
are about as simple as it is possible for an argument form to be, so they seem
like a good place to begin.

Modus Ponens and Modus Tollens

:   1.  *Modus Ponens* (abbreviated MP), the argument form

        $\phi\rightarrow \psi~.~\phi\therefore\psi$

        is a rule of direct inference.

    2.  *Modus Tollens* (abbreviated MT), the argument form

        $\phi\rightarrow\psi~.~\sim\psi\therefore\sim\phi$

        is a rule of direct inference.

Notice that we are using a different kind of symbol here, to describe argument
forms. The letters $\phi$ and $\psi$ are from the Greek alphabet. We use them
because the modus ponens and modus tollens argument forms do not have anything
to do with any particular sentence letters. Rather, they are *logical forms*
that particular arguments may have. Any argument that you get by substituting
actual sentences for $\phi$ and $\psi$ in the form
$\phi\rightarrow\psi~.~\phi\therefore\psi$ is deductively valid, and is an
example of a modus ponens argument.

The following arguments are all examples of the modus ponens argument form:

#.  $P\rightarrow Q~.~P\therefore Q$ \hfill (filling in $P$ for $\phi$, $Q$ for $\psi$)
#.  $Q\rightarrow P~.~Q\therefore P$ \hfill (filling in $Q$ for $\phi$, $P$ for $\psi$)
#.  $(Q\land R)\rightarrow P~.~(Q\land R)\therefore P$ \hfill ($(Q\land R)$ for $\phi$, $P$ for $\psi$)
#.  $Q\rightarrow (P\land R)~.~Q\therefore (P\land R)$  \hfill ($Q$ for $\phi$, $(P\land R)$ for $\psi$)

The following arguments are all examples of the modus tollens argument form:

#.  $P\rightarrow Q~.~\sim Q\therefore \sim P$ \hfill (filling in $P$ for $\phi$, $Q$ for $\psi$)
#.  $Q\rightarrow P~.~ \sim P\therefore \sim Q$ \hfill (filling in $Q$ for $\phi$, $P$ for $\psi$)
#.  $(Q\land R)\rightarrow P~.~ \sim P\therefore \sim (Q\land R)$  \hfill ($(Q\land R)$ for $\phi$, $P$ for $\psi$)
#.  $Q\rightarrow (P\land R)~.~ \sim(P\land R) \therefore \sim Q$  \hfill ($Q$ for $\phi$, $(P\land R)$ for $\psi$)

We will also begin with two other rules of direct inference. 

Double Negation

:   The argument form 

    $\phi\therefore\sim\sim\phi$

    and the argument form 

    $\sim\sim\phi\therefore\phi$ 
    
    are both rules of direct inference, knonw as the *Double Negation* (DN) rules
    (also as *Double Negation Introduction* (DNI) and *Double Negation Elimination* (DNE),
    Respectively).

These will be all of our initial rules of direct inference. More rules will be
introduced as we proceed.

##Direct Derivations

In order to use a direct derivation to show something we will need to keep
track of a  couple of things. First of all, we will need to keep track of what
we are trying to show, so that others can know what we are intending to do.
Second of all, we will need to keep track of what we have already shown, since
each new step must be based on previous steps or premises. Third, we will need
to keep track of the justification for each step that we are making, so that
we, and others, can easily verify the correctness of each step in our
reasoning.

Suppose we are given an argument, with some premises and a conclusion. Let us
keep track of what we intend to show by writing "Show", followed by the
conclusion that we are aiming to show follows from the premises. In order to
keep track of what we have already show, let's organize our assertions into
a list, and give each assertion a number. Finally, to keep track of the
justification for each step we are making, let's write an abbreviated
explanation to the right of the assertion. This explanation can be either
"$Pr$", meaning that the assertion is one of the premises of the argument, or
it can be the numbers of the previous lines which serve as premises for a rule
of direct inference justifying it, followed by the name of the rule. Finally,
when we have finished and managed to produce the assertion which we are trying
to show, let's write down the number of the line where we produced this final
assertion, and the abbreviation "$DD$", meaning "I have produced this assertion
by direct derivation".[^1]

[^1]: Where should we put this final justification for closing the box? We
clearly have many choices. I will place it as a little label on the box being
closed. In the homework, you will use it as a justification on an otherwise
blank line to close a box. See the screencasts for more details.

We can then enclose our derivation in a box to keep it together, and cross out
the word "Show" to indicate that what we were aiming to show has now been shown
(this is called "canceling" the show line). The result will look like this:

```{#img3 .latex .tikz}
\begin{KMcalc}
\KMprove{$\phi$}{}
\KMline{\vdots}{Justification}
\KMline{\vdots}{Justification}
\KMline{\vdots}{Justification}
\KMline{\vdots}{Justification}
\KMline{$\phi$\qquad}{Justification}
\KMclose[6 DD]
\end{KMcalc}
```

Though, of course, the number of lines is unimportant. 

For example, suppose we are trying to use a direct derivation to show that the
argument $P~.~P\rightarrow Q~.~Q\rightarrow R\therefore R$ is valid.

We begin with a show line, saying that we want to show $R$. We then write down
the premises $P, P\rightarrow Q,Q\rightarrow R$ on lines 2-4. Finally we use
modus ponens ($MP$) to get $Q$ on line 5 from the premises $P$ and
$P\rightarrow Q$---so we write $Q$ on line 5, and a justification ($MP$) to the
right, with the line numbers of the premises $P$ (line 2) and $P\rightarrow Q$
(line 3). We do the same thing, on the basis of premises $Q$ and $Q\rightarrow
R$ to assert $R$ on line $6$. We then box the result, and label that we have
found $Q$ on the basis of a direct derivation, by writing $6 DD$ and crossing
out the show line. The result looks like this:

```{#img4 .latex .tikz}
\begin{KMcalc}
\KMprove{$R$}{}
\KMline{$P$}{PR}
\KMline{$P\rightarrow Q$}{PR}
\KMline{$Q\rightarrow R$}{PR}
\KMline{$Q$}{2 3 MP}
\KMline{$R$}{4 5 MP}
\KMclose[6 DD]
\end{KMcalc}
```

Here is another example. Suppose we are trying to use a direct derivation to
show that the argument $P\rightarrow Q~.~\sim P \rightarrow R~.~\sim
Q\therefore R$ is valid. We begin with a show line. We then write down the
premises $P\rightarrow Q, \sim P\rightarrow R, \sim Q$ on lines 2-4. Using
Modus Tollens, we can infer $\sim P$ from $\sim Q$ and $P\rightarrow Q$. So we
do this, writing down $\sim P$ on line 5, and indicating that we are using
modus tollens (MT) on lines $2,4$ to get this result. But using $\sim P$, and
$\sim P\rightarrow R$, we can get $R$ using Modus Ponens. So we write down $R$
on line 6, and indicate the justification for this assertion beside it. The
result looks like this:

```{#img5 .latex .tikz}
\begin{KMcalc}
\KMprove{$R$}{}
\KMline{$P\rightarrow Q$}{PR}
\KMline{$\sim P\rightarrow R$}{PR}
\KMline{$\sim Q$}{PR}
\KMline{$\sim P$}{2 4 MT}
\KMline{$R$}{5 3 MP}
\KMclose[6 DD]
\end{KMcalc}
```

One more example. Suppose we are trying to use a direct derivation to show that
the argument $\sim P\rightarrow Q~.~\sim Q \therefore P$ is valid. We begin with
a show line. We then write down the premises $\sim P\rightarrow Q$ and $\sim Q$
on lines 2-3. We can use modus tollens with these premises to write down
$\sim\sim P$ on line 4. Then, on line 5, we can write down $P$, using the
assertion on line 4, plus the rule of double negation elimination (which we may
cite as just DN). The derivation looks like this:

```{#img6 .latex .tikz}
\begin{KMcalc}
\KMprove{$R$}{}
\KMline{$\sim P\rightarrow Q$}{PR}
\KMline{$\sim Q$}{PR}
\KMline{$\sim\sim P$}{2 3 MT}
\KMline{$P$}{4 DN}
\KMclose[5 DD]
\end{KMcalc}
```

##Conditional Derivations

Direct Derivations are not the only kinds of derivations. There are, in fact,
two more basic types of derivations we will consider. The first of these is
called a **Conditional Derivation**. It is a derivation which aims to derive
a conditional statement by *assuming* the assertion on the  left side of the
conditional, and then using whatever means are available to reach the statement
on the right side of the conditional. What you are doing is imagining, for
a moment, that the statement on the left is true, and seeing what else would be
true in that case.

This process may be familiar from, for example, hypothetical questions on exams.

When you know the answer to a hypothetical question like this one:

> Suppose that a NASCAR race car is moving to the right with a constant
velocity of $+82 m/s$. What is the average acceleration of the car?

you probably don't know anything about any particular actual car (after all,
you have no idea of the current velocity of any NASCAR car). What you do know,
if you know the answer, is that *if* a NASCAR race car is moving to the right
with a constant velocity of $+82 m/s$, *then* the average acceleration of the
car is zero $m/s^2$.

It may also be familiar with assuming something temporarily, to figure out what
would happen if it were true, from games of strategy, like Tic Tac Toe,
Checkers, Chess, Go, and others. In games like this, it is often useful to be
able to predict what will happen if we make a certain move.

You probably realize that in this situation:

<div style="display: table; margin: 0 auto">

```{#img7 .latex .tikz}
\begin{tikzpicture}
\draw (0,0) -- (0,3);
\draw (1,0) -- (1,3);
\draw (-1,1) -- (2,1);
\draw (-1,2) -- (2,2);
\draw (.5,2.5) circle (10pt);
\draw (1.25,2.25) -- (1.75, 2.75);
\draw (1.25,2.75) -- (1.75, 2.25);
\draw (-.25,2.25) -- (-.75, 2.75);
\draw (-.25,2.75) -- (-.75, 2.25);
\end{tikzpicture}
```

</div>

where it is $X$'s turn, *if* $X$ makes a move in the middle spot, then $X$ can
win on the next turn. How do you know this? This is, after all, in effect an
ability to predict the future. Even though it a common sort of
future-prediction, it might seem rather mysterious how we are able to
accomplish this.[^2]

[^2]:Many people tend to believe that when we make
predictions about the future, we do so only because we generalize from
a pattern encountered in our previous experience. This view is often thought to
be common sense nowadays partly because, about sixty or seventy years ago, it
was a popular view among a group of philosophers called *Logical Empiricists*
(The economist John Maynard Keynes said that "even the most practical man
of affairs is usually in the thrall of the ideas of some long-dead
economist."---the same goes doubly for people who pride themselves on their
common sense, and the ideas of long-dead philosophers). Here is one reason for
thinking that this cannot be a matter of generalizing from previous experience.
You have never played most games of tic tac toe, for there are 26,830 possible
games of tic tac toe, (counting games that result from other games by rotation
or reflection only once). In other games where strategic thinking is important,
the same is true. Experience simply cannot be expected to have acquainted you
with the types of situations you will face. For example, there are about
$10^{120}$ possible 40-move games of chess (this is called Shannon's number,
after Claude Shannon "the father of information theory", who made the estimate). Only
an infinitesimal fraction of these games will ever be played; far fewer have
ever been experienced by even the best chess players.

The trick---you may realize if you attend to your thought process---is that you
*imagine* $X$ actually making the critical move into the center position. Then,
you think about what could happen in this situation. You'll quickly realize
that no matter what move $O$ makes on the next turn, $X$ will be able to
connect three. So, In this hypothetical situation $X$ can win on the next turn.
Hence, you realize, *if* $X$ makes a move in the middle spot, then $X$ can win
on the next turn

Arguably, this type of thinking---imagining yourself in a hypothetical
scenario, and reasoning about what would be true in that scenario in order to
gather "conditional information" about the real world---is common in many
ordinary activities as well.[^3]

[^3]:Some clever examples of hypothetical reasoning can also be found in
fiction. For example, the chess house fight in the movie *Hero*, and the final
confrontation between Holmes and Moriarty in *Sherlock Holmes, Game of Shadows*
both depict something like conditional reasoning. Interestingly, both of those
two scenes reference games of strategy---Sherlock has just played a game of
chess with Moriarty, and Sky is just finishing a game of go.

We can represent the abstract structure of conditional reasoning as a (simple)
conditional derivation:

Conditional Derivation

:   A (simple) *conditional derivation* is a sequence of assertions

    #.  Aimed at showing a conditional $\phi\rightarrow\psi$

    #.  beginning with an assumption that $\phi$ is true 

    #.  in which every assertion other than the first justified, either because
    it is a  premise or because it is the conclusion of a rule of direct
    inference with previous lines as premises.

To use a conditional derivation to show something, first of all, we will need
to keep track of what we are trying to show, so that others can know what we
are intending to do. Second of all, we will need to keep track of what we are
assuming, and what we are actually justifying on the basis of other things.
Third of all, we will need to keep track of what we have already shown, since
each new step must be based on previous steps or premises. Fourth, we will need
to keep track of the justification for each step that we are making, so that
we, and others, can easily verify the correctness of each step in our
reasoning.

We'll keep track of most of these things using the same tools that we did for
direct derivations: we will write "Show" to indicate what we are showing, we
will number our lines, and indicate where the premises to rules MP,MT, and DN
are coming from by using line numbers. There will only be three real
differences. 

First, we will aim only to show "if\ldots then" conditional statements. Second,
we will begin each derivation by assuming the statement on the left side of the
"if\ldots then" statement that we are aiming to show; we will write ASS CD to
justify our assumption. Third, we will consider ourselves finished, box the
derivation and cancel (i.e. cross out) the show line when we manage to assert
the statement on the right side of the conditional; we will write the number of
the line where we asserted the statement on the right side, together with CD,
to mean "I have shown the conditional statement by means of a conditional
derivation ending with this line". The result will look like this:

```{#img8 .latex .tikz}
\begin{KMcalc}
\KMprove{$\phi\rightarrow\psi$}{}
\KMline{$\phi$}{ASS CD}
\KMline{$\vdots$}{Justification}
\KMline{$\vdots$}{Justification}
\KMline{$\vdots$}{Justification}
\KMline{$\psi\qquad$}{Justification}
\KMclose[6 CD]
\end{KMcalc}
```

Here are some examples of conditional derivations.

For the argument $P\rightarrow Q~.~ Q\rightarrow R \therefore P\rightarrow R$, we can derive:

```{#img9 .latex .tikz}
\begin{KMcalc}
\KMprove{$P\rightarrow R$}{}
\KMline{$P$}{ASS CD}
\KMline{$P\rightarrow Q$}{Pr}
\KMline{$Q\rightarrow R$}{Pr}
\KMline{$Q$}{2,3 MP}
\KMline{$R$}{4,5 MP}
\KMclose[6 CD]
\end{KMcalc}
```

For the argument $P\rightarrow(\sim Q\rightarrow R)~.~\sim R \therefore
P\rightarrow Q$, we can derive

```{#img10 .latex .tikz}
\begin{KMcalc}
\KMprove{$P\rightarrow Q$}{}
\KMline{$P$}{ASS CD}
\KMline{$P\rightarrow (\sim Q\rightarrow R)$}{Pr}
\KMline{$\sim R$}{Pr}
\KMline{$\sim Q\rightarrow R$}{2,3 MP}
\KMline{$\sim\sim Q$}{4,5 MT}
\KMline{$Q$}{6 DN}
\KMclose[7 CD]
\end{KMcalc}
```

For the argument $P\rightarrow (Q\rightarrow (R\rightarrow S))~.~\sim
Q\rightarrow \sim R~.~R\therefore P\rightarrow S$, we can derive

```{#img1 .latex .tikz}
\begin{KMcalc}
\KMprove{$P\rightarrow S$}{}
\KMline{$P$}{ASS CD}
\KMline{$P\rightarrow (Q\rightarrow (R\rightarrow S))$}{Pr}
\KMline{$\sim Q\rightarrow \sim R$}{Pr}
\KMline{$R$}{Pr}
\KMline{$Q\rightarrow (R\rightarrow S)$}{2 3 MP}
\KMline{$\sim\sim R$}{5 DN}
\KMline{$\sim\sim Q$}{4 7 MT}
\KMline{$Q$}{8 DN}
\KMline{$R\rightarrow S$}{9 6 MP}
\KMline{$S$}{10 5 MP}
\KMclose[10 CD]
\end{KMcalc}
```
