---
course_title: Symbolic Logic 1
note_title: Weeks 4-6
has_toc: Yes
css: logic.css
---

#New Propositional Derivations

##Nested Derivations

Sometimes, a conditional derivation will not be enough to show that a certain 
conditional we want is true. For example, conditionals of the form $P → (Q → 
R)$ will be difficult to prove. This situation is reflected in ordinary 
reasoning.

We may, while playing chess, want to know whether a statement like “If I take 
the queen, then if my opponent moves her bishop to 8D she can put me in check” 
is true. Just imagining that we take the queen is not enough for us to figure 
out what will happen if we take the queen AND our opponent moves her bishop. 
We need to first imagine that we take the queen. We then need to add another 
piece of information to that hypothetical scenario—we suppose that our 
opponent then moves her Bishop.

What we are doing is adding another “layer” of hypothesis to our imaginary 
situation. Just as our first imaginary situation was like the real world 
except for one addition (that we took the queen), our second imaginary 
situation is just like our first imaginary situation, but with one addition 
(that our opponent moves her bishop).

It is natural to model this type of “double assumption”, by allowing for 
derivations to occur inside of other derivations. Let’s call a derivation that 
has other derivations occurring inside of it a Nested Derivation. And let’s 
divide the nested derivations we know how to construct into two types: Nested 
Conditional Derivations, and Nested Direct Derivations.

Nested Direct Derivation
:   

    A Nested Direct Derivation is is a sequence of assertions, aimed at 
    showing some statement φ, each of which is justified, either because

    (a). it is a premise of the argument under consideration;

    (b). it is the conclusion of a rule of direct derivation applied to previ- 
         ous assertions;

    (c). or, it is listed next to a cancelled show-line, meaning that it has 
         been shown to be true by a derivation.

Nested Conditional Derivation
:   

    A Nested Conditional Derivation is is a sequence of assertions aimed at 
    showing some statement φ → ψ, beginning with an assumption that φ, in 
    which every assertion after the first is justified, either because

    (a). it is a premise of the argument under consideration;

    (b). it is the conclusion of a rule of direct derivation applied to 
         previous assertions;

Nested Direct Derivations and Nested Conditional Derivations are much the same 
as Simple Direct Derivations and Simple Conditional Derivations. In Nested 
Direct Derivations, we make a series of assertions, all of which are true, 
basing later assertions on earlier assertions. In Nested Conditional 
Derivations, we make an assumption, and then make a series of assertions that 
would be true, if that assumption were true, basing later assertions on 
earlier assertions.

The only difference between the nested and the simple cases is that in the 
nested case, we allow a new way of basing later assertions on earlier 
assertions. In addition to rules of direct inference, we allow for an 
assertion to be made on the basis of a earlier assertions if that assertion 
can be *shown*, by a derivation which uses those earlier assertions.

Here are some examples of nested derivations.

For the argument $P\rightarrow (Q\rightarrow R)~.~ \neg Q\rightarrow S \therefore P\rightarrow (\neg S\rightarrow R)$, we can derive:

```{#img2 .latex .tikz}
\begin{KMcalc}
\KMprove{$ P\rightarrow (\neg S\rightarrow )$}{}
\KMline{$P$}{ASS CD}
\KMline{$P\rightarrow (Q\rightarrow R)$}{PR}
\KMline{$\neg Q\rightarrow S$}{PR}
\KMline{$Q\rightarrow R$}{2 3 MP}
\KMprove{$\neg S\rightarrow R$}{}
\KMline{$\neg S$}{ASS CD}
\KMline{$\neg\neg Q$}{4 7 MT}
\KMline{$Q$}{8 DN}
\KMclose[9 CD]
\KMclose[6 CD]
\end{KMcalc}
```

For the argument $(P\rightarrow R)\rightarrow S~.~ P\rightarrow Q~.~ Q\rightarrow R\therefore S$, we can derive:

```{#img3 .latex .tikz}
\begin{KMcalc}
\KMprove{$S$}{}
\KMline{$(P\rightarrow R)\rightarrow S$}{Pr}
\KMline{$P\rightarrow Q$}{Pr}
\KMline{$Q\rightarrow R$}{Pr}
\KMprove{$P\rightarrow R$}{}
\KMline{$P$}{ASS CD}
\KMline{$Q$}{3 6 MP}
\KMline{$R$}{4 7 MP}
\KMclose[8 CD]
\KMline{$S$}{2 5 MP}
\KMclose[9 DD] \end{KMcalc}
```

##Indirect Derivations

The last type of derivation that we will consider is an indirect derivation. 
An indirect derivation is, generally speaking, a derivation used in order to 
show that something is *not* true. How do we ordinarily know that something is 
not true? One thing that we do is to reject statements that conflict with 
things that we already know to be true.

For example:

1.  You get a call from the elementary school, saying that your little brother 
    was absent. Later, you ask: where were you this afternoon? Your brother 
    says: I was at school. You then know that what your bother says is false.

2.  You know that the weight tolerance of a certain kind of steel—glass 
    steel—is fairly low. Things build out of this steel cannot bear a lot of 
    weight. But the steel is expensive because it withstands cold very well. A 
    contractor builds a structure, which turns out to be carrying an 
    incredible amount of weight. You ask: what’s the building material? They 
    say “why, it’s 100% glass steel”. Then—if you’re not wrong about something 
    else—you know the contractor is saying something false.

3. You happen to know that, to ski down Mount Terror, the suspect would have 
   needed to buy some skiing gear. You also happen to know that if the suspect 
   spent the day skiing down Mount Terror, then the suspect had no time to buy 
   any skiing gear. You ask the suspect: "Where were you today?" They say 
   “why, I was skiing down Mount Terror!”. Then you know the suspect is lying 
   to you.

Let’s take a closer look at this last case. How do we know the suspect is 
lying? Well, suppose the suspect did go skiing down mount terror. Then given 
what we know, they’d have to have bought some skis. But given what we know, 
they also could not have bought some skis. So, under the assumption that the 
suspect did go skiing down mount terror, they both did and did not buy skis. 
In any situation compatible with our knowledge, where the suspect did go 
skiing, they both bought and did not buy skis. But such a situation is absurd, 
impossible. So we can safely conclude on the basis of what we know that the 
suspect did not, in fact, go skiing down Mount Terror.

We can represent the abstract structure of this sort of reasoning as a 
(simple) Indirect Derivation

Indirect Derivation

:   A (simple) indirect derivation is a sequence of assertions

    1. Aimed at showing a statement φ is false, or equivalently that ∼φ is
    true.

    2. beginning with an assumption that φ is true

    3. in which every assertion other than the first justified, either because it 
    is a premise or because it is the conclusion of a rule of direct inference 
    with previous lines as premises.

To use an indirect derivation to show something, we’ll need to do many of the 
same things that we needed to do to show something with a conditional 
derivation. First of all, we will need to keep track of what we are trying to 
show, so that others can know what we are intending to do. Second of all, we 
will need to keep track of what we are assuming, and what we are actually 
justifying on the basis of other things. Third of all, we will need to keep 
track of what we have already shown, since each new step must be based on 
previous steps or premises. Fourth, we will need to keep track of the 
justification for each step that we are making, so that we, and others, can 
easily verify the correctness of each step in our reasoning.

As before, we will write “Show” to indicate what we are showing, we will 
number our lines, and indicate where the premises to rules MP,MT, and DN are 
coming from by using line numbers.

For simple indirect derivations, we will aim only to show negative statements 
of the form ∼φ. Second, we will begin each derivation by assuming the 
statement φ; we will write ASS ID to justify our assumption. Third, we will 
consider ourselves finished, box the derivation and cancel (i.e. cross out) 
the show line when we manage to assert both some statement ψ and the negation 
∼ψ of ψ. We will write the numbers of the line where we asserted these two 
statements, together with ID, to mean “I have shown the conditional statement 
by means of a conditional derivation ending with this line.

The result will look like this:

```{#img4 .latex .tikz}
\begin{KMcalc}
\KMprove{$\neg\phi$}{}
\KMline{$\phi$}{ASS ID}
\KMline{$\vdots$}{Justification}
\KMline{$\vdots$}{Justification}
\KMline{$\psi$}{Justification}
\KMline{$\vdots$}{Justification}
\KMline{$\neg\psi\qquad$}{Justification}
\KMclose[4 6 ID]
\end{KMcalc}
```

The particular numbers do not matter, of course, and neither does whether 
$\psi$ or $\neg\psi$ comes first in the derivation.

There are two variations on indirect derivations that we can introduce. First 
of all, indirect derivations can also include some nested show lines.

Nested Indirect Derivation
:   

    A nested indirect derivation derivation is a sequence of assertions, aimed 
    at showing ∼φ, beginning with the assumption that φ, each of which after 
    the first is justified because either

    1. it is a premise of the argument under consideration;

    2. it is the conclusion of a rule of direct derivation applied to previous 
       assertions;

    3. it is listed next to a cancelled show-line, meaning that it has been 
       shown to be true by a derivation.

Second we can allow a special kind of indirect derivation, which is some- 
times easier to find than an ordinary indirect derivation (even though it 
proves nothing that cannot be proven in the ordinary way). We can call this a 
indirect derivation "of the second form", or a "reverse" indirect derivation: 
a reverse indirect derivation is a derivation in which we find a 
contradiction, just like with an ordinary indirect derivation, but in which we 
aim to show φ, rather than ∼φ, and in which we allow ourselves to assume ∼φ, 
rather than φ. Why can we do this? Well, in fact, we can already do it, by 
constructing a nested indirect derivation, like so:

```{#img5 .latex .tikz}
\begin{KMcalc}
\KMprove{$\phi$}{}
\KMprove{$\neg\neg\phi$}{}
\KMline{$\neg\phi$}{ASS ID}
\KMline{$\vdots$}{Justification}
\KMline{$\vdots$}{Justification}
\KMline{$\psi$}{Justification}
\KMline{$\vdots$}{Justification}
\KMline{$\neg\psi\qquad$}{Justification}
\KMclose[6 8 ID]
\KMline{$\phi$}{2 DN}
\KMclose[9 DD]
\end{KMcalc}
```
Whenever we're in a position to get a contradiction by assuming $\neg\phi$, 
we're in a position to prove $\neg\neg\phi$ by indirect derivation. We can 
then always prove $\phi$ by double negation elimination.

We therefore allow reverse indirect derivations as a kind of short-cut, so that we can avoid the clutter of that extra nested proof. A simple reverse indirect derivation will look like this:

```{#img6 .latex .tikz}
\begin{KMcalc}
\KMprove{$\phi$}{}
\KMline{$\neg\phi$}{ASS ID}
\KMline{$\vdots$}{Justification}
\KMline{$\vdots$}{Justification}
\KMline{$\psi$}{Justification}
\KMline{$\vdots$}{Justification}
\KMline{$\neg\psi\qquad$}{Justification}
\KMclose[5 7 ID]
\end{KMcalc}
```

We'll allow for nested reverse indirect derivations as well.

Nested Reverse Indirect Derivation
:   

    A nested reverse indirect derivation derivation is a sequence of 
    assertions, aimed at showing φ, beginning with the assumption that ∼φ, 
    each of which after the first is justified because either

    1. it is a premise of the argument under consideration;

    2. it is the conclusion of a rule of direct derivation applied to previous 
       assertions;

    3. it is listed next to a cancelled show-line, meaning that it has been 
       shown to be true by a derivation. 

#New Rules

You may have noticed that the derivations we have constructed so far only use 
the symbols → and ∼. This keeps things simpler, and gives us a chance to 
introduce conditional and indirect derivations before we have too much else to 
remember. But now that we can construct conditional and indirect derivations, 
it’s time to begin to improve our simple machinery. Our improvements will 
allow us to find derivations that show the validity of arguments involving all 
of our connectives: not just → and ∼, but also ∧, ∨, and ↔

##Repetition

The first new rule we will introduce seems trivial. It is the rule of 
repetition:

Repetition
:   Repetition (abbreviated as R), the argument form abbreviated

    $\phi\therefore\phi$

    Is a rule of direct Inference.

This rule is rather odd in many ways. But it serves a purpose in our system. 
In order to close a conditional derivation of a statement 
$\phi\rightarrow\psi$, it's necessary to have $\psi$ *beneath* the show line 
that you are trying to close. If you have a $\psi$ somewhere up above, then 
repetition lets you move it to the appropriate place. Similarly, if you wish 
to close an indirect derivation of a statement $\neg\phi$, it's necessary to 
have contradictory statements $\psi,\sim\psi$ beneath the show line. If you 
have, for example, $\psi$, under the show line, and $\sim\psi$ somewhere else, 
then repetition lets you move $\sim\psi$ down to where it is needed.

It's interesting to note that it is also possible to move statements down in 
another way. If you have $\phi$ somewhere in a derivation, and you want it at 
the bottom, you can first apply double negation to $\phi$, to get 
$\sim\sim\phi$ at the bottom of your proof, then apply double negation to 
$\sim\sim\phi$ to get $\phi$ at the bottom. So the rule of repetition, like 
reverse indirect derivation, is strictly-speaking never needed. It doesn't let 
you do anything new. But it lets you do something you already could do in a 
faster way. So we allow it in order to make things simpler.

##Rules for the remaining connectives

When we have the premises P,∼Q, we can derive the conclusion ∼(P → Q).

This is because there’s a hidden contradiction in the three assertions P,∼Q,P 
→ Q. In English, this is pretty clear: if someone were to say to you on one 
occasion “Sam’s going to the beach” on another “Sam’s not going to borrow any 
lemonade from your fridge” and on a third occasion “if Sam goes to the beach, 
Sam is going to borrow some lemonade”, then you’d know that something they 
said was false.

If we happened to know that “Sam’s going to the beach” was true, and also that 
“Sam’s not going to borrow any lemonade” was true, then we could conclude that 
“If Sam goes to the beach, I’m going to borrow some lemonade” must be the one 
of the three assertions that’s false.

That is essentially what we do in an indirect proof when we show that from P 
(which might be “Sam’s going to the beach”) and ∼Q (which might be “Sam’s not 
going to borrow any lemonade”) it follows that ∼(P → Q) (which would then mean 
“it’s not the case that if Sam’s going to the beach then Sam’s going to borrow 
some lemonade”).

But to make our reasoning explicit in an indirect derivation, we need to show 
how the statements P,∼Q,P → Q cannot all be true together—we need to find the 
contradiction that’s hidden within them. That’s why we need Modus Ponens, in 
the following argument:

```{#img9 .latex .tikz}
\begin{KMcalc}
\KMprove{$\neg (P\rightarrow Q)$}{}
\KMline{$P\rightarrow Q$}{ASS ID}
\KMline{$P$}{Pr}
\KMline{$\neg Q$}{Pr}
\KMline{$Q$}{2 3 MP}
\KMclose[4 5 ID]
\end{KMcalc}
```

So, in this case, Modus Ponens Helps us uncover a hidden contradiction. 

Modus Ponens and Modus Tollens help us uncover contradictions hidden in 
statements involving $\rightarrow$, and the double-negation rules help us 
uncover contradictions hidden in statements involving $\neg$. What are some 
other examples of types of hidden contradictions that we may wish to uncover?

Here are two examples of contradictions hidden in a sentence involving “and”.
The following cannot be true together:

1. “I was at home the night of the crime, and I have the receipts to prove it!”
2. “I was not at home the night of the crime, and because I was out with a 
   friend, I have someone who will vouch for me!”

But why not? Because the first two clauses “I was at home the night of the 
crime” and “I was not at home the night of the crime” are contradictory. It 
looks as if the right rule to make this contradiction explicit is one that 
lets us separate off the two sides of an “and” statement, and regard each one 
as something that someone who utters an “and” statement is committed to.
The following cannot be true together:

1. “I have a cousin”
2. “I have a pet iguana”
3. “I don’t have both a cousin and a pet iguana”

But why not? Because the first two sentences, when you put them together, 
contradict the third.[^1] It looks as if the right rule to make this 
contradiction explicit is one that lets us put together two statements to 
which someone is committed, by combining them with an “and”, and then to view 
the person as committed to the result.

[^1]: Although, interestingly, there’s no contradiction between any two of the 
sentences without the third—the contradiction is “spread out” evenly across 
the three sentences

So this example suggests that the following two rules ought to be added to our system, to let us deal with the connective ∧, which represents “and”.

Simplification and Adjunction

:   

    1.  Adjunction (abbreviated Adj), the argument form
        
        $\phi~.~\psi\therefore\phi\land\psi$

        Is a rule of direct inference

    2. Simplification (abbreviated S), the argument which takes the form

        $\phi\land\psi\therefore\phi$

        or

        $\phi\land\psi\therefore\psi$

        is a rule of direct inference.

What about the rules for ∨? What do we need to chase out contradictions when 
they involve the logic of the word “or”?

Consider this example: the following can’t be true together: 

1. Sam is not at the beach.
2. Sam is not at home.
3. Sam is at home or at the beach

Again, the contradiction is spread across three sentences. The second two 
sentences, put together, contradict the third, since if Sam is at home or at 
the beach, and we rule out Sam being at home, then Sam must be at the beach. 
What we need in order to uncover the contradiction is a way of using a 
negation to “rule out” one side of an “or” statement, and rule the other side 
in.

And, the following can’t be true together:

1. I have a pet iguana
2. I don’t have either a pet iguana or a beach house.

The trouble here is that if you have a pet iguana, then you certainly have a 
pet iguana or a beach house. And there’s a contradiction between that and the 
claim “I don’t have either a pet iguana or a beach house”. What we need in 
this case is a rule that lets us go from “I have a pet iguana” to “I have a 
pet iguana or I have a beach house”. This isn’t a very common sort of 
inference to make when we are trying to build on our own knowledge. But it is 
an important inference to have on hand when we are trying to chase out 
contradictions into the open, as we are in this case.

These examples suggest that we ought to also add the following two rules, to 
deal with ∨, which represents “or”:

Modus Tollendo Ponens and Addition

:   
    1.  Modus Tollendo Ponens (abbreviated MTP), the argument which takes the 
        form

        $\phi\lor\psi~.~\sim\phi\therefore\psi$

        or

        $\phi\lor\psi~.~\sim\psi\therefore\phi$

        is a rule of direct inference.

    2.  Addition (abbreviated ADD), the argument which takes the form

        $\phi\therefore\phi\lor\psi$

        or

        $\phi\therefore\psi\lor\phi$

        is a rule of direct inference.

We now have two rules (ADJ and S) for “and”, and two rules (MTP and ADD) for “or”.

What sorts of rules should we adopt for “↔”? Well, what this connective means 
is “if and only if”. When we say “I will come to the party if and only if 
there’s some beer”, what we’re saying is that the presence of beer is both a 
necessary condition (because we will come only if there’s some beer) and a 
sufficient condition (because we will come if there’s some beer) for us coming 
to the party. So the “if and only if” statement is a kind of “and” statement. 
In the case above, we’re saying “I will come to the party if there’s beer, and 
I will come to the party only if there’s beer”.

From our work on formalization, we know that this means “If there’s beer, then 
I will come to the party, and if I come to the party, then then there’s beer”. 
So our “and” rules already tell us what to do. When we have the “if and only 
if” statement, we can break off either half, inferring either that “If there’s 
beer, then I will come to the party” or that “If I come to the party, then 
there’s beer”. And if we have both halves, we can put them together to get the 
“and” statement.

Conditional-Biconditional, Biconditional Conditional

:   
    1.  Conditional-Biconditional (abbreviated CB), the argument which takes 
        the form

        $\phi\rightarrow\psi~.~\psi\rightarrow\phi\therefore\phi\leftrightarrow\psi$

        is a rule of direct inference.

    2.  Biconditional Conditional (abbreviated BC), the argument which takes 
        the form

        $\phi\leftrightarrow\psi\therefore\phi\rightarrow\psi$

        or

        $\phi\leftrightarrow\psi\therefore\psi\rightarrow\phi$

        is a rule of direct inference.

We now have rules for ↔, ∧ and ∨. Together with the rules we already have for 
∼ and for →, these will be all we need to handle the whole language that we 
are working in.

